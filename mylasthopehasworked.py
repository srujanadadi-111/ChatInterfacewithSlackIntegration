# -*- coding: utf-8 -*-
"""mylasthopehasworked.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y2KIwG2CbNDmQQHTifLkuZGORv7IZELm
"""

!pip install chromadb slack_sdk boto3 reportlab openai pymupdf nltk

!pip install chromadb pymupdf nltk

slack_bot_token = os.environ.get('SLACK_BOT_TOKEN')

# Set OpenAI API key
SLACK_BOT_TOKEN = slack_bot_token

openai_api_key = os.environ.get('OPENAI_API_KEY')

# Set OpenAI API key
OPENAI_API_KEY = openai_api_key

"""Getting info from slack channels"""

import datetime
from collections import defaultdict
from slack_sdk import WebClient
import chromadb
import os

# --- CONFIGURATION ---
slack_token = os.environ.get('SLACK_BOT_TOKEN')
if not slack_token:
    raise ValueError("Please set the SLACK_BOT_TOKEN environment variable.")

client = WebClient(token=slack_token)

# --- CHROMA SETUP ---
chroma_client = chromadb.PersistentClient(path="/content/chroma_db")
collection = chroma_client.get_or_create_collection("hcmbot_knowledge")

def get_all_joined_channel_ids():
    channel_ids = []
    cursor = None
    while True:
        response = client.conversations_list(types="public_channel,private_channel", limit=1000, cursor=cursor)
        for channel in response['channels']:
            if channel.get('is_member'):
                channel_ids.append(channel['id'])
        cursor = response.get('response_metadata', {}).get('next_cursor')
        if not cursor:
            break
    return channel_ids

def fetch_hcmsupportbot_messages_all_channels(oldest, latest):
    all_messages = []
    channel_ids = get_all_joined_channel_ids()
    for channel_id in channel_ids:
        try:
            response = client.conversations_history(
                channel=channel_id,
                oldest=oldest,
                latest=latest,
                limit=1000
            )
            for msg in response['messages']:
                if '#hcmsupportbot' in msg.get('text', '').lower():
                    all_messages.append({
                        'channel': channel_id,
                        'user': msg.get('user', ''),
                        'text': msg.get('text', ''),
                        'ts': float(msg.get('ts')),
                        'thread_ts': msg.get('thread_ts', msg.get('ts'))
                    })
        except Exception as e:
            print(f"Error fetching from {channel_id}: {e}")
    return all_messages

def group_messages_by_thread(messages):
    threads = defaultdict(list)
    for msg in messages:
        threads[msg['thread_ts']].append(msg)
    for thread in threads.values():
        thread.sort(key=lambda m: m['ts'])
    return threads

def create_slack_chunks(messages):
    threads = group_messages_by_thread(messages)
    chunks = []
    for thread_msgs in threads.values():
        chunk_text = "\n".join([f"{m['user']}: {m['text']}" for m in thread_msgs])
        if chunk_text.strip():
            chunks.append({
                "text": chunk_text,
                "metadata": {
                    "source": "slack",
                    "channel": thread_msgs[0]["channel"],
                    "thread_ts": thread_msgs[0]["thread_ts"],
                    "date": datetime.datetime.fromtimestamp(thread_msgs[0]["ts"]).strftime("%Y-%m-%d")
                }
            })
    return chunks

def main():
    now = datetime.datetime.now()
    yesterday = now - datetime.timedelta(days=1)
    messages = fetch_hcmsupportbot_messages_all_channels(
        oldest=int(yesterday.timestamp()),
        latest=int(now.timestamp())
    )
    if messages:
        chunks = create_slack_chunks(messages)
        for i, chunk in enumerate(chunks):
            doc_id = f"slack_{chunk['metadata']['thread_ts']}_{i}"
            # Avoid duplicate ingestion
            try:
                existing = collection.get(ids=[doc_id])
                if existing['ids']:
                    print(f"Chunk {doc_id} already exists, skipping.")
                    continue
            except Exception:
                pass
            collection.add(
                documents=[chunk["text"]],
                metadatas=[chunk["metadata"]],
                ids=[doc_id]
            )
            print(f"Ingested Slack thread as chunk {doc_id}.")
        #chroma_client.persist()
        print(f"\nInserted {len(chunks)} Slack threads into ChromaDB and persisted.")
    else:
        print("No messages found for today.")

main()

"""Getting data from drive - base pdfs"""

import os
import fitz  # PyMuPDF
import chromadb
import nltk
import datetime
import glob

# Download NLTK sentence tokenizer
nltk.download('punkt')
nltk.download('punkt_tab')

# Mount Google Drive if not already mounted
from google.colab import drive
drive.mount('/content/drive')

# Initialize ChromaDB PersistentClient
client = chromadb.PersistentClient(path="/content/chroma_db")
collection = client.get_or_create_collection("hcmbot_knowledge")

def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text

def split_text_into_chunks(text, max_tokens=500, overlap=100):
    sentences = nltk.tokenize.sent_tokenize(text)
    chunks = []
    current_chunk = []
    current_length = 0
    for sentence in sentences:
        sentence_length = len(sentence.split())
        if current_length + sentence_length > max_tokens:
            chunk_text = ' '.join(current_chunk)
            chunks.append(chunk_text)
            overlap_words = chunk_text.split()[-overlap:]
            current_chunk = [' '.join(overlap_words), sentence]
            current_length = len(overlap_words) + sentence_length
        else:
            current_chunk.append(sentence)
            current_length += sentence_length
    if current_chunk:
        chunks.append(' '.join(current_chunk))
    return chunks

def ingest_pdf_to_chroma(pdf_path):
    chunks = split_text_into_chunks(extract_text_from_pdf(pdf_path))
    base_name = os.path.basename(pdf_path)
    for i, chunk in enumerate(chunks):
        doc_id = f"{base_name}_{i}"
        # Avoid duplicate ingestion
        try:
            existing = collection.get(ids=[doc_id])
            if existing['ids']:
                print(f"Chunk {doc_id} already exists, skipping.")
                continue
        except Exception:
            pass
        collection.add(
            documents=[chunk],
            metadatas=[{
                "source": "base_pdf",
                "filename": base_name,
                "chunk_index": i,
                "date": datetime.datetime.now().strftime("%Y-%m-%d")
            }],
            ids=[doc_id]
        )
        print(f"Ingested chunk {i} from {base_name} to ChromaDB.")

# Path to your Google Drive folder with PDFs
folder_path = '/content/drive/MyDrive/Upload/pdfs2'  # Change to your folder!

# Ingest all PDFs in the folder
pdf_files = glob.glob(os.path.join(folder_path, '*.pdf'))
print(f"Found {len(pdf_files)} PDFs in {folder_path}")

for pdf_path in pdf_files:
    ingest_pdf_to_chroma(pdf_path)

# Persist to disk
#client.persist()
print("All PDFs ingested and ChromaDB persisted.")

"""Printing all chunks in Chromadb"""

!pip install chromadb

import chromadb

client = chromadb.PersistentClient(path="/content/chroma_db")
collection = client.get_or_create_collection("hcmbot_knowledge")

results = collection.get(include=["documents", "metadatas"])

ids = results.get('ids', [])

print(f"\nTotal documents: {len(results['documents'])}")

base_pdf_count = 0
slack_count = 0

for i, (doc, meta) in enumerate(zip(results['documents'], results['metadatas'])):
    doc_id = ids[i] if i < len(ids) else 'unknown'
    source = meta.get("source", "unknown")
    if source == "base_pdf":
        base_pdf_count += 1
    elif source == "slack":
        slack_count += 1
    print(f"\n--- Document {i+1} ---")
    print(f"ID: {doc_id}")
    print(f"Source: {source}")
    print(f"Metadata: {meta}")
    print(f"Content snippet: {doc[:200]}...")

print(f"\nSummary:")
print(f"Base PDF chunks: {base_pdf_count}")
print(f"Slack chunks: {slack_count}")

if base_pdf_count > 0 and slack_count > 0:
    print("\n‚úÖ Both base PDFs and Slack data are present in your ChromaDB!")
elif base_pdf_count > 0:
    print("\n‚ö†Ô∏è Only base PDFs found. Slack data missing!")
elif slack_count > 0:
    print("\n‚ö†Ô∏è Only Slack data found. Base PDFs missing!")
else:
    print("\n‚ùå No documents found. Check your ingestion process.")

"""Tests: Direct content search - is it even there?!"""

import re

def direct_content_search(collection, search_term, case_insensitive=True, use_regex=False):
    results = collection.get(include=["documents", "metadatas"])
    found = []
    for doc, meta in zip(results["documents"], results["metadatas"]):
        if use_regex:
            flags = re.IGNORECASE if case_insensitive else 0
            if re.search(search_term, doc, flags=flags):
                found.append((doc, meta))
        else:
            haystack = doc.lower() if case_insensitive else doc
            needle = search_term.lower() if case_insensitive else search_term
            if needle in haystack:
                found.append((doc, meta))
    return found

search_term = "Srujana's bday is on May 20th."
matches = direct_content_search(collection, search_term)
for i, (doc, meta) in enumerate(matches):
    print(f"--- Match {i+1} ---")
    print("Source:", meta.get("source", "unknown"))
    print("Metadata:", meta)
    print("Content snippet:", doc[:400])
    print("---")

search_term = "Type command: (to check which files are not downloaded)"
matches = direct_content_search(collection, search_term)
for i, (doc, meta) in enumerate(matches):
    print(f"--- Match {i+1} ---")
    print("Source:", meta.get("source", "unknown"))
    print("Metadata:", meta)
    print("Content snippet:", doc[:400])
    print("---")

search_term = "The magic test phrase is: purple-elephant-9876"
matches = direct_content_search(collection, search_term)
for i, (doc, meta) in enumerate(matches):
    print(f"--- Match {i+1} ---")
    print("Source:", meta.get("source", "unknown"))
    print("Metadata:", meta)
    print("Content snippet:", doc[:400])
    print("---")

import chromadb

client = chromadb.PersistentClient(path="/content/chroma_db")  # Adjust path if needed
collection = client.get_or_create_collection("hcmbot_knowledge")

def hybrid_retrieve_chunks(query, top_k=5):
    results = collection.query(
        query_texts=[query],
        n_results=top_k * 3,
        include=["documents", "metadatas"]
    )
    docs = results["documents"]
    metas = results["metadatas"]

    # Flatten if docs/metas are lists of lists
    flat_docs = []
    flat_metas = []
    for doc, meta in zip(docs, metas):
        if isinstance(doc, list):
            flat_docs.extend(doc)
        else:
            flat_docs.append(doc)
        if isinstance(meta, list):
            flat_metas.extend(meta)
        else:
            flat_metas.append(meta)

    query_terms = set(query.lower().split())
    scored = []
    for doc, meta in zip(flat_docs, flat_metas):
        doc_terms = set(doc.lower().split())
        overlap = len(query_terms & doc_terms)
        substring_match = any(term in doc.lower() for term in query_terms)
        score = overlap + (1 if substring_match else 0)
        scored.append((score, doc, meta))

    scored.sort(reverse=True, key=lambda x: x[0])
    return [(doc, meta) for score, doc, meta in scored[:top_k]]

# Example usage:
results = hybrid_retrieve_chunks("The magic test phrase", top_k=10)
for rank, (doc, meta) in enumerate(results):
    print(f"Rank: {rank+1}")
    print("Source:", meta.get("source", "unknown"))
    print("Metadata:", meta)
    print("Content snippet:", doc[:400])
    print("---")

def print_hybrid_results(results):
    for rank, (doc, meta) in enumerate(results):
        print(f"Rank: {rank+1}")
        if isinstance(meta, dict):
            print("Source:", meta.get("source", "unknown"))
        elif isinstance(meta, list):
            # If meta is a list, print all sources in the list
            sources = [m.get("source", "unknown") if isinstance(m, dict) else "unknown" for m in meta]
            print("Source (list):", sources)
        else:
            print("Source: unknown")
        print("Metadata:", meta)
        print("Content snippet:", doc[:400])
        print("---")

results = hybrid_retrieve_chunks(query, top_k=10)
print_hybrid_results(results)

"""App.py - LLM prompt"""

import chromadb
import os

# --- ChromaDB Setup ---
client = chromadb.PersistentClient(path="/content/chroma_db")  # Adjust path if needed
collection = client.get_or_create_collection("hcmbot_knowledge")

# --- Retrieval Function ---
def hybrid_retrieve_chunks(query, top_k=5):
    results = collection.query(
        query_texts=[query],
        n_results=top_k * 3,
        include=["documents", "metadatas"]
    )
    docs = results["documents"]
    metas = results["metadatas"]

    # Flatten if docs/metas are lists of lists
    flat_docs = []
    flat_metas = []
    for doc, meta in zip(docs, metas):
        if isinstance(doc, list):
            flat_docs.extend(doc)
        else:
            flat_docs.append(doc)
        if isinstance(meta, list):
            flat_metas.extend(meta)
        else:
            flat_metas.append(meta)

    query_terms = set(query.lower().split())
    scored = []
    for doc, meta in zip(flat_docs, flat_metas):
        doc_terms = set(doc.lower().split())
        overlap = len(query_terms & doc_terms)
        substring_match = any(term in doc.lower() for term in query_terms)
        score = overlap + (1 if substring_match else 0)
        scored.append((score, doc, meta))

    scored.sort(reverse=True, key=lambda x: x[0])
    return [(doc, meta) for score, doc, meta in scored[:top_k]]

# --- (Optional) LLM Answer Synthesis ---
from openai import OpenAI

def chat_with_assistant(query, docs, model="gpt-3.5-turbo"):
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", ""))
    context = "\n\n".join(docs)
    prompt = f"""You are a precise assistant that answers questions based strictly on the provided context.
Rules:
1. Use ONLY information from the context
2. Keep exact terminology and steps from the source
3. If multiple sources have different information, specify which source you're using
4. If information isn't in the context, say "I don't have enough information"
5. For procedures, list exact steps in order
6. Include specific buttons, links, and UI elements mentioned in the source.

Context:
{context}

Question: {query}
Answer:"""
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        max_tokens=500,
        temperature=0.3
    )
    return response.choices[0].message.content


def main():
    question = input("Enter your question: ")
    results = hybrid_retrieve_chunks(question, top_k=3)
    docs = [doc for doc, meta in results]

    for rank, (doc, meta) in enumerate(results):
        print(f"Rank: {rank+1}")
        print("Source:", meta.get("source", "unknown"))
        print("Metadata:", meta)
        print("Content snippet:", doc[:400])
        print("---")

    # Synthesized LLM answer (now compulsory)
    try:
        answer = chat_with_assistant(question, docs)
        print("\n--- LLM Synthesized Answer ---")
        print(answer)
    except Exception as e:
        print("LLM error:", str(e))

if __name__ == "__main__":
    main()

# --- Streamlit Interface ---
if 'question_clicks' not in st.session_state:
    st.session_state.question_clicks = {
        "What is Health Campaign Management?": 0,
        "What are the steps involved in creating a KPI?": 0
    }
if 'query' not in st.session_state:
    st.session_state.query = ""

def handle_trending_click(question):
    st.session_state.question_clicks[question] += 1
    st.session_state.query = question

#st.image("egovlogo.png", width=200)
st.title("Health Campaign Management (HCM) Support Bot [Beta version]")

st.subheader("Note:")
st.markdown(
    '<p style="color:red; font-size:16px;">Please try to be as in detail as possible with your prompt and use full forms for beta version, e.g., Health Campaign Management instead of HCM.</p>',
    unsafe_allow_html=True,
)

st.subheader("Trending Questions")
col1, col2 = st.columns(2)
with col1:
    for question in list(st.session_state.question_clicks.keys())[:1]:
        if st.button(f"üìà {question}", key=f"btn_{question}"):
            handle_trending_click(question)
with col2:
    for question in list(st.session_state.question_clicks.keys())[1:]:
        if st.button(f"üìà {question}", key=f"btn_{question}"):
            handle_trending_click(question)

# User input section
query = st.text_input("Ask a question:", value=st.session_state.query, key="query_input")
submit_button = st.button("Submit")

if submit_button or query:
    if query.strip():
        st.write("Query Received:", query)
        docs, metas, ids = retrieve_chunks(query)
        st.write("**Top relevant knowledge snippets:**")
        for i, (doc, meta, doc_id) in enumerate(zip(docs, metas, ids)):
            st.markdown(f"**{i+1}. Source:** {meta.get('source', 'unknown')}")
            st.markdown(f"`{doc_id}`")
            st.markdown(f"> {doc[:400]} ...")
            st.markdown("---")
        # LLM answer (optional)
        if os.environ.get("OPENAI_API_KEY", "").startswith("sk-"):
            try:
                answer = chat_with_assistant(query, docs)
                st.success(f"Assistant's answer: {answer}")
            except Exception as e:
                st.error(f"LLM error: {str(e)}")
        else:
            st.info("Set your OPENAI_API_KEY as an environment variable to enable LLM answers.")
    else:
        st.warning("Please enter a question before clicking Submit.")
